\documentclass[12pt]{article}
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{bm}
\usepackage{epsfig}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{tabls}
\usepackage{hhline}
\usepackage{float}
\usepackage{subfigure}
%\usepackage{subfigmat}
\usepackage{citesort}
%\usepackage{cites}
\usepackage{overcite}
                                                                                                            
% uncomment for submission of manuscript to NSE
\usepackage[nolists, nomarkers]{endfloat}
%
% use to include postscript figures
\usepackage{graphicx}
%
%\usepackage[light,firsttwo]{draftcopy}
%\draftcopySetGrey{0.90}
                                                                                                            
%\usepackage{dbl}

% -----------------------------------------------------------------------------
% define newcommands
% -----------------------------------------------------------------------------

%\setlength{\floatsep}{4pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{8pt plus 1pt minus 1pt}
%\setlength{\intextsep}{4pt plus 1pt minus 1pt}
\setlength{\abovedisplayskip}{4pt plus 1pt minus 1pt}
\setlength{\belowdisplayskip}{4pt plus 1pt minus 1pt}

\makeatletter
\renewcommand{\@thesubfigure}{\thefigure\thesubfigure\space}
\makeatother
% =================================================================================================
% more new commands
% +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{0pt}
\setlength{\headsep}{12pt}
%\addtolength{\oddsidemargin}{-0.5in}
%\addtolength{\textwidth}{1.0in}
%\addtolength{\textheight}{1.0in}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%
% -----------------------------------------------------------------------------
% define newcommands
% -----------------------------------------------------------------------------

%\setlength{\floatsep}{4pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{8pt plus 1pt minus 1pt}
%\setlength{\intextsep}{4pt plus 1pt minus 1pt}
\setlength{\abovedisplayskip}{4pt plus 1pt minus 1pt}
\setlength{\belowdisplayskip}{4pt plus 1pt minus 1pt}

% =================================================================================================
% more new commands
% +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% Ways of grouping things
%
\newcommand{\bracket}[1]{\left[ #1 \right]}
\newcommand{\bracet}[1]{\left\{ #1 \right\}}
\newcommand{\fn}[1]{\left( #1 \right)}
\newcommand{\ave}[1]{\left\langle #1 \right\rangle}
%
% Derivative forms
%
\newcommand{\dx}[1]{\,d#1}
\newcommand{\dxdy}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dxdt}[1]{\frac{\partial #1}{\partial t}}
\newcommand{\dxdz}[1]{\frac{\partial #1}{\partial z}}
\newcommand{\dfdt}[1]{\frac{\partial}{\partial t} \fn{#1}}
\newcommand{\dfdz}[1]{\frac{\partial}{\partial z} \fn{#1}}
\newcommand{\ddt}[1]{\frac{\partial}{\partial t} #1}
\newcommand{\ddz}[1]{\frac{\partial}{\partial z} #1}
\newcommand{\dd}[2]{\frac{\partial}{\partial #1} #2}
\newcommand{\ddx}[1]{\frac{\partial}{\partial x} #1}
\newcommand{\ddy}[1]{\frac{\partial}{\partial y} #1}
%
% Vector forms
%
%\renewcommand{\vec}[1]{\ensuremath{\stackrel{\rightarrow}{#1}}}
%\renewcommand{\div}{\ensuremath{\vec{\nabla} \cdot}}
%\newcommand{\grad}{\ensuremath{\vec{\nabla}}}
\renewcommand{\vec}[1]{\overrightarrow{#1}}
\renewcommand{\div}{\vec{\nabla}\! \cdot \!}
\newcommand{\grad}{\vec{\nabla}}
\newcommand{\oa}[1]{\fn{\frac{1}{3}\hat{\Omega}\!\cdot\!\overrightarrow{A_{#1}}}}

%
% Equation beginnings and endings
%
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\beas}{\begin{eqnarray*}}
\newcommand{\eeas}{\end{eqnarray*}}
\newcommand{\bdm}{\begin{displaymath}}
\newcommand{\edm}{\end{displaymath}}
%
% Equation punctuation
%
\newcommand{\pec}{\hspace{0.25in},}
\newcommand{\pep}{\hspace{0.25in}.}
\newcommand{\pev}{\hspace{0.25in}}
%
% Equation labels and references, figure references, table references
%
\newcommand{\LEQ}[1]{\label{eq:#1}}
\newcommand{\EQ}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\EQS}[1]{Eqs.~(\ref{eq:#1})}
\newcommand{\REQ}[1]{\ref{eq:#1}}
\newcommand{\LFI}[1]{\label{fi:#1}}
\newcommand{\FI}[1]{Fig.~\ref{fi:#1}}
\newcommand{\RFI}[1]{\ref{fi:#1}}
\newcommand{\LTA}[1]{\label{ta:#1}}
\newcommand{\TA}[1]{Table~\ref{ta:#1}}
\newcommand{\RTA}[1]{\ref{ta:#1}}

%
% List beginnings and endings
%
\newcommand{\bl}{\bss\begin{itemize}}
\newcommand{\el}{\vspace{-.5\baselineskip}\end{itemize}\ess}
\newcommand{\ben}{\bss\begin{enumerate}}
\newcommand{\een}{\vspace{-.5\baselineskip}\end{enumerate}\ess}
%
% Figure and table beginnings and endings
%
\newcommand{\bfg}{\begin{figure}}
\newcommand{\efg}{\end{figure}}
\newcommand{\bt}{\begin{table}}
\newcommand{\et}{\end{table}}
%
% Tabular and center beginnings and endings
%
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\btb}{\begin{center}\begin{tabular}}
\newcommand{\etb}{\end{tabular}\end{center}}
%
% Single space command
%
%\newcommand{\bss}{\begin{singlespace}}
%\newcommand{\ess}{\end{singlespace}}
\newcommand{\bss}{\singlespacing}
\newcommand{\ess}{\doublespacing}
%
%---New environment "arbspace". (modeled after singlespace environment
%                                in Doublespace.sty)
%   The baselinestretch only takes effect at a size change, so do one.
%
\def\arbspace#1{\def\baselinestretch{#1}\@normalsize}
\def\endarbspace{}
\newcommand{\bas}{\begin{arbspace}}
\newcommand{\eas}{\end{arbspace}}
%
% An explanation for a function
%
\newcommand{\explain}[1]{\mbox{\hspace{2em} #1}}
%
% Quick commands for symbols
%
\newcommand{\half}{\frac{1}{2}}
\newcommand{\third}{\frac{1}{3}}
\newcommand{\twothird}{\frac{2}{3}}
\newcommand{\mdot}{\dot{m}}
\newcommand{\ten}[1]{\times 10^{#1}\,}
\newcommand{\cL}{{\cal L}}
\newcommand{\cD}{{\cal D}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cS}{{\cal S}}
\newcommand{\mA}{\mathbf{A}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\mW}{\mathbf{W}}
\newcommand{\mSigma}{\mathbf{\Sigma}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\mB}{\mathbf{B}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\mD}{\mathbf{D}}
\renewcommand{\Re}{\mbox{Re}}
\newcommand{\Ma}{\mbox{Ma}}
%
% Inclusion of Graphics Data
%
%\input{psfig}
%\psfiginit
%
% More Quick Commands
%
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\dxi}{\Delta x_i}
\newcommand{\dyj}{\Delta y_j}
\newcommand{\ts}[1]{\textstyle #1}
% =================================================================================================
\date{}

\begin{document}

%\bibliographystyle{nse}
%\bibnum{p}

\thispagestyle{empty}

\bss
\bc
{\Large \bf Dynamic Mode Decomposition for Subcritical Metal Systems}\\
\vspace{0.3in}
{\large Zachary Hardy and Jim E. Morel\\
Texas A\&M University\\
Department of Nuclear Engineering\\
TAMU 3133\\
College Station, TX 77843-3133\\
$ $\\
Cory Ahrens\\
Los Alamos National Laboratory\\
Los Alamos, NM  87545\\
$ $\\
}
\emph{zach.hardy@tamu.edu, morel@tamu.edu, cdahrens@lanl.gov}

\vspace{1.5in}

Send proofs and page charges to:\\
\vspace{0.1in}
Dr. Jim Morel\\
Texas A\&M University\\
Department of Nuclear Engineering\\
TAMU 3133\\
College Station, TX 77843-3133\\
\vspace{0.25in}
25 Pages -- 1 Table -- 4 Figures \\
\ec
\ess

% =================================================================================================

\newpage

\begin{abstract}
In this paper we explore the use of Dynamic Mode Decomposition (DMD) for modeling the kinetics of 
subcritical metal systems pulsed with fast neutrons.  Our ultimate purpose is to obtain a fast and 
accurate reduced-order model for such systems that can be used to develop an emulator.  
An alternative to DMD is $\alpha$-eigenfunction expansions, but we show that DMD is vastly superior in 
several ways for the systems of interest to us. 
\end{abstract}

\section{Introduction}
Modeling of subcritical metal systems subjected to short bursts of fast neutrons is challenging. 
While such modeling can certainly be done with a time-dependent transport code, we want to develop a
reduced-order model that can provide physical insight into such systems and form the basis of an emulator in 
order to solve inverse problems quickly and compute sensitivities economically. 

One method of developing a reduced-order model is a truncated $\alpha$-eigenfunction expansion. To the author's knowledge, no proof of the
incompleteness of the alpha eigenfunctions exists. It seems, however, that because of the singular nature of the uncollided and first
collided flux (cite Larsen and Bondarenko), the $\alpha$-eigenfunctions likely do not form a complete basis. However, Jorgens proved, under
 rather general assumptions, that the $\alpha$-eigenfunctions form an ``asymptotically complete'' basis in the following sense: for times $t>3\tau$, where $\tau$ is a
representative time for neutrons to stream across the spatial domain, $\psi\left(t\right) \sim \sum_{n=0}^{N-1} A_n \psi_n e^{\alpha_n t}$, where the
remainder term is proportional to $e^{\alpha_N t}$ and the amplitudes $A_n$ depend on the initial condition. From this representation and the fact that the
eigenvalues are ordered as $...Re\left(\alpha_3\right)\le Re\left(\alpha_2\right)\le Re\left(\alpha_1\right) < \alpha_0$, one sees that $|\alpha_0 -
\alpha_1|^{-1}$ sets a time scale for the higher-order modes to decay relative to the fundamental mode. The relationship of this time scale compared with 
the time over which the experiment takes place can be significantly different for subcritical compared to supercritical systems.  

Indeed, for a supercritical system, where $\alpha_0 > 0$ and the real part of $\alpha_n$, $n=1,2,3,...$ is negative, the transport
solution is quickly dominated by the fundamental mode $\alpha$-eigenfunction. However, the 
situation can be different for a subcritical system. If the flux (experimental signal) decays much faster than $|\alpha_0 -
\alpha_1|^{-1}$ , then the fundamental mode might not contribution significanlty to the signal before it is negligibly small. The contribution of the 
fundamental mode depends upon the extent to which it is present in the expansion for the initial condition. In 
a subcritical system the fundamental mode will be dominated by the slowest neutrons, which will be thermal 
neutrons. In a metal system subjected to a burst of fast neutrons, there will be almost no thermal neutrons 
created before the response of the system has effectively died away.  Thus it seems likely that an $\alpha$-eigenfunction expansion 
will be particularly inefficient for such systems, because a high degree of cancellation between the eigenfunctions 
will be required to achieve a negligible thermal neutron component in the initial condition.  

Dynamic Mode Decomposition (DMD) is a reduced order technique for modeling dynamical systems.  It has been applied 
in many areas (need references) and is perhaps best known for its use in the fluid-flow community.  The purpose of 
this paper is to perform a preliminary investigation of DMD as an alternative to $\alpha$-eigenfunction expansions 
for modeling pulsed neutron experiments. (Need to reference any previous neutronics work with DMD here.) 
For this initial study, we use a very approximate but relevant 
high-dimensional model consisting of a time-dependent 1-D spherical geometry three-group diffusion approximation. 
We have an analytic eigenfunction solution for these equations, as well as a computer code for solving 
these equations with second-order accuracy in both time and space.  We later describe DMD in detail, but at 
this point it suffices to say that given a time series of vector ``snapshots'' from a simulation or experiment, 
DMD produces a time-dependent solution for those snapshots that is constructed from a sum of snapshot modes, each with 
an exponential decay rate.  For instance, the snapshots could be space-dependent, three-group diffusion scalar fluxes 
from a time-dependent calculation.  They could also be an analogous time series of any quantity of interest computed 
from the diffusion solution, such as space-dependent reaction rates.  The form of the DMD solution is identical to that 
of an alpha-eigenfunction solution.  However, the DMD modes can only contain what the snapshots contain, so if the snapshots 
lack a thermal neutron component, so will the DMD modes.  This suggests that fewer DMD modes should be required for 
our problems than $\alpha$-eigenfunctions.

Indeed we present results demonstrating that the DMD method is accurate and more efficient than 
$\alpha$-eigenfunction expansions for our problems.  The remainder of this paper is organized as follows.  First we describe 
the particular variant of the DMD method that we use.  Then we describe the three-group diffusion model, followed by a 
description of our space-time discretization for the diffusion model.  Finally, numerical results are given, followed by 
conclusions and recommendations for future work.


\section{The DMD Method}
There are many variations of the DMD method.  Here we describe the variant that we use to model subcritical fissioning systems.  
One begins with a time series of vector snapshots, $\{\vec{v}_j\}_{j=1}^{N}$, uniformly sampled in time, that are associated with the 
dynamical system.  We assume that at least the first $N-1$ snapshots are linearly-independent. A process for ensuring this 
is later discussed.  Each vector has $M$ real 
components with $M \gg N$. For example, each snapshot could represent a discrete angular flux solution in a time-dependent 
transport calculation, or it could represent some space-dependent 
quantity of interest associated with that solution.  Note that the snapshots can be obtained either computationally or 
experimentally. For our application, we generate them computationally. It is further assumed that there exists a temporal 
matrix $\mathbf{A}$, that maps each snapshot to its successor:
\be
\vec{v}_{i+1} = \mA \vec{v}_{i} \pec \quad i=1,N-1.
\LEQ{2.1}
\ee
Let us consider the subspace of $M$-vectors, which we denote by $\cS$, spanned by the first $N-1$ snapshots.  If $\vec{v}_N$ 
were replaced by a least-squares fit from $\cS$, then $\mA$ would be uniquely defined as a mapping from $\cS$ to $\cS$, and 
be represented by the following matrix with respect to the snapshot basis:
\be
\mA_s = \bracket{
\begin{array}{ccccccc}
0 & 0 & ....& 0 & c_1 \\
1 & 0 & ....& 0 & c_2 \\
0 & 1 & ....& 0 & c_3 \\
. & . & ....& . & . \\
. & . & ....& . & . \\
. & . & ....& . & . \\
0 & 0 & ....& 0 & . \\
0 & 0 & ....& 1 & c_{N-1}
\end{array}
} \pec
\LEQ{2.2}
\ee
where the least-squares fit to $\vec{v}_N$ from $\cS$ is represented by 
\be
\widehat{v}_N = \sum_{j=1}^{N-1} c_j \vec{v}_j \pep
\LEQ{2.3}
\ee
For example, in the snapshot basis, 
\be
\vec{v}_1 = (1,0,0,0,...0)^T \pec
\LEQ{2.4}
\ee
and 
\be
\mA \vec{v}_1 = \vec{v}_2 = (0,1,0,0,...0)^T \pec
\LEQ{2.5}
\ee
Note that if $c_1$ is non-zero, $\mA$ will be invertible on $\cS$, and thus will have $N-1$ non-zero eigenvalues. 
We will define $\mA$ in this manner with respect to $\cS$, assume that it is invertible on $\cS$, and further define 
all of its remaining eigenvalues to be zero.  
Let $\{\lambda_i\}_{i=1}^{N-1}$ denote the non-zero eigenvectors of $\mA$, and let $\{\vec{z}_i\}_{i=1}^{N-1}$ denote the 
corresponding eigenvectors. Then the dynamic solution is given by 
\be
\vec{v}(t) = \sum_{i=1}^{N-1} a_i \vec{z}_i \exp{(\omega_i t)} \pec
\LEQ{2.5a}
\ee
where 
\be
\omega_i = \frac{\ln{(\lambda_i)}}{\Delta t} \pec
\LEQ{2.5b}
\ee
where $\Delta t$ is the time between snapshots and the expansion coefficients, $\{a_i\}_{i=1}^{N-1}$ are determined by the 
initial condition:
\be
\vec{v}(0) = \vec{v}_1 = \sum_{i=1}^{N-1} a_i \vec{z}_i \pep
\LEQ{2.5c}
\ee
The dynamic solution exactly reproduces each of the snapshots at its time of sampling with the 
exception of the last snapshot, which is approximated with its least-squares fit. This follows from the 
fact that at the end of the first sampling period, $\mA$ is effectively applied to $\vec{v}_1$, 
and reapplied at the end of each sampling period thereafter.  For instance, 
\bea
\vec{v}(\Delta t) &=& \sum_{i=1}^{N-1} a_i \vec{z}_i \exp{(\omega_i \Delta t)} \pec \nonumber \\
&=& \sum_{i=1}^{N-1} a_i \vec{z}_i \lambda_i \pec \nonumber \\ 
&=& \mA \vec{v}_1 \pec \nonumber \\ 
&=& \vec{v}_2 \pep
\LEQ{2.5d}
\eea 
Note that any eigenfunction with a zero-eigenvalue will have no dynamic content because it will 
instantly attenuate to zero.  Thus if $\widehat{v}_N$ has a zero value for $c_1$ in \EQ{2.3},
$\mA$ will have only $N-2$ non-zero eigenvalues, and one can simply ignore the eigenfunction associated 
with the extra zero eigenvalue.  We henceforth continue to assume that $\mA$ has $N-1$ non-zero eigenvalues 
for simplicity, but without loss of generality.

We now describe a process that enables us to compute the non-zero eigenvalues and the eigenvectors of 
of $\mA$.  Furthermore, we need not explicitly compute $\widehat{v}_N$.   First we 
express \EQ{2.1} as follows, neglecting the substitution of $\widehat{v}_N$ for $\vec{v}_N$ for reasons later explained. 
\be
\mX_2^{N} = \mA \mX_{1}^{N-1} \pec
\LEQ{2.6}
\ee
where $\mX_2^{N}$ is the $M \times N-1$ matrix whose colunms are the vectors $\vec{v}_2$ through $\vec{v}_{N}$, 
and $\mX_{1}^{N-1}$ is the $M\times N-1$ matrix whose colunms are the vectors $\vec{v}_1$ through 
$\vec{v}_{N-1}$. 

We next perform a singular value decomposition (SVD) of $\mX_{1}^{N-1}$:
\be
\mX_{1}^{N-1} = \mU \mSigma \mW^T \pec
\LEQ{2.7}
\ee
where $\mU$ is a $M \times M$ orthogonal matrix, $\mSigma$ is a $M\times N-1$ matrix with $N-1$ non-zero singular values on 
the diagonal and zeros everywhere else, and $\mW^T$ is a 
$N-1 \times N-1$ orthogonal matrix.  Substituting from \EQ{2.6} into \EQ{2.7}, we obtain, 
\be
\mX_2^{N} = \mA \mU \mSigma \mW^T \pec
\LEQ{2.8}
\ee
Next we multiply \EQ{2.8} on the left by $\mU^{T}$ 
and on the right by $\mW\mSigma^{-1}$ to obtain 
\be
\mS = \mU^{T}\mX_2^{N}\mW\mSigma^{-1} = \mU^{T}\mA\mU \pec
\LEQ{2.10}
\ee
where $\mSigma^{-1}$ is the pseudo inverse of $\mSigma$.  
The pseudo-inverse is obtained from $\mSigma$ simply by first inverting its non-zero elements and then transposing it. 
Note that the right side of \EQ{2.10} is a $M \times M$ matrix that represents a similarity transformation 
of $\mA$, and thus $\mS$ has the same eigenvalues as $\mA$.  Furthermore, in accordance with the similarity transformation, 
if $\vec{y}$ is an eigenvector of $\mS$, then $\vec{z} = \mU \vec{y}$ is an eigenvector of $\mA$.  

The structure of $\mS$ is important because it enables us to avoid computing the entire $M \times M$ matrix together with 
its eigenvalues and eigenvectors.  More specifically, all the columns of $\mS$ beyond column $N-1$ are zero, and 
we represent the remainder of the matrix as an $N-1 \times N-1$ matrix, $\mS_{T}$, in the upper top left corner and 
a $M-N+1 \times N-1$ matrix, $\mS_B$, in the bottom left corner:
\be
\mS = \bracket{
\begin{array}{cc}
\mS_T  & 0 \\
\mS_B & 0  
\end{array}
} \pec
\LEQ{2.11}
\ee
It can be shown that the eigenvalues of $\mS_T$ are the non-zero eigenvalues of $\mS$, and hence, the non-zero eigenvalues of 
$\mA$.  Furthermore, there is a very simple way to compute the corresponding eigenvectors of $\mS$ from the eigenvectors of 
$\mS_T$. However, it is unnecessary to compute any eigenvectors other than those of $\mS_T$.  To explain why this is so, we 
first note that the there is no difference between the eigenvectors of $\mS_T$ and the vectors formed by the first $N-1$ 
components of the corresponding eigenvectors of $\mS$.  Thus if one computes the eigenvalues and eigenvectors of $\mS_T$, 
one has the non-zero eigenvalues of $\mA$ and $\mS$, and the corresponding eigenvectors of $\mS$ truncated to a length of $N-1$.

The first step in the calculation of the non-zero eigenvalues and corresponding eigenvectors of $\mA$ is to directly 
compute $\mS_T$ as follows:
\be
\mS_T = \mU_{N-1}^{T} \mX_{2}^{N}\mW\mSigma^{-1}_{N-1} \pec
\LEQ{2.12}
\ee
where $\mU_{N-1}$ is the $M \times N-1$ matrix composed of the first $N-1$ columns of $\mU$, and $\mSigma_{N-1}$ is the 
$N-1 \times N-1$ matrix formed by the first $N-1$ rows of $\mSigma$.  

At this point we have sufficient information to explain why one need not substitute $\widehat{v}_N$ for $\vec{v}_N$ in 
$\mX_2^{N}$.  To this end we express the equations for the expansion coefficients of the least-squares fit to $\vec{v}_N$ 
as follows:
\be
\mU^T_{N-1}\fn{\widehat{v}_N - \vec{v}_N} = \vec{0} \pec
\LEQ{2.9}
\ee
where $\mU_{N-1}$ is the $M \times N-1$ matrix consisting of the first $N-1$ columns of $\mU$. Note that by virtue of the 
SVD of $\mX_{1}^{N-1}$, these column vectors represent an orthonormal basis for the first $N-1$ snapshots.  Thus $\widehat{v}_N$ 
does indeed represent a least-squares fit to $\vec{v}_N$ from $\cS$.  From \EQ{2.9} it follows that the first $N-1$ components of 
$\mU^T \vec{v}_N$ are identical to those of $\mU^T \widehat{v}_N$. Thus it can be seen from \EQ{2.12} that $\mS_T$ is 
invariant to the substitution of $\widehat{v}_N$ for $\vec{v}_N$ in $X_{2}^{N}$.

Finally, we compute the non-zero eigenvectors of $\mA$ simply by multiplying the eigenvectors of $\mS_T$ by $U_{N-1}$. 
\be
\vec{z}_i = \mU_{N-1} \vec{x}_i \pec \quad i=1,N-1,
\LEQ{2.13}
\ee
where $\vec{z}_i$ and $\vec{x}_i$ are the $i$'th eigenvectors of $\mA$ and $\mS_T$, respectively.  This is clearly much more 
economical than computing the eigenvectors of $\mS$ and multiplying them by $\mU$.  This simplification is justified by the fact 
that since the first $N-1$ columns of $\mU$ form an orthonormal basis for $\cS$, the non-zero eigenvectors of $\mA$ 
must lie in this space.  Thus columns of $\mU$ beyond $N-1$, or equivalently, elements of the eigenvectors of $\mS$ below the 
$N-1$ position do not contribute to the non-zero eigenvectors of $\mA$. 

The SVD of $\mX_{1}^{N-1}$ can be used to determine if the first $N-1$ snapshots are sufficiently linearly independent by 
inspection of the singular values.  Below some tolerance one can consider the singular values to be zero, and discard the 
corresponding information in the various component matrices.  More specifically, if we assume that only the first $K$ 
singular values are considered non-zero, then in \EQ{2.12} only the first $K$ columns of $\mU_{N-1}$ and $\mX_{2}^{N}$ are retained, 
and only the first $K$ columns and first $K$ rows of $\mSigma_{N-1}$ and $\mW^T$ are retained.  We currently set any singular 
value that is less than $10^{-8}$ times the largest singular value to zero. It is not generally clear as to what tolerance is 
the most efficient.  We determine this on a case-by-case basis.

\section{The Three-Group Diffusion Model}

\section{Discretization of the Diffusion Equations}

\section{Numerical Results}

\section{Conclusions and Recommendations for Future Work}

%\begin{thebibliography}{99}

%\end{thebibliography}

\end{document} 

